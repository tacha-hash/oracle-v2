# Session Retrospective

**Date**: 2026-01-06
**Time**: 19:13 - 19:45 GMT+7
**Duration**: ~32 minutes
**Focus**: Vector search fix + Knowledge graph connectivity improvements

## Summary

Fixed ChromaDB vector search in the HTTP server using claude-mem's architecture pattern, then analyzed the knowledge graph to discover 272 unconnected nodes (63% of total). Implemented frontmatter tag parsing and expanded keywords to dramatically improve graph connectivity from 4,928 to 20,118 links (+308%).

## What We Built

### Commits
1. `785d9e5` - feat: Add vector search to HTTP server using claude-mem pattern
2. `9506aca` - feat: Add frontmatter tags + expanded keywords for graph connectivity

### Key Changes

**Vector Search (`src/chroma-mcp.ts`, `src/server/handlers.ts`)**:
- ChromaMcpClient with auto-reconnect on "Not connected" errors
- Hybrid search combining FTS + vector results
- Cosine distance normalization (0-2 → 0-1 similarity)
- Hybrid scoring: `max(fts, vector) + 0.1` bonus

**Graph Connectivity (`src/indexer.ts`)**:
- `parseFrontmatterTags()` - extracts `tags:` from markdown frontmatter
- `mergeConceptsWithTags()` - inherits file-level tags to all chunks
- Expanded keywords: 18 → 36 common terms

### Results

| Metric | Before | After |
|--------|--------|-------|
| Connected nodes | 159 | 313 |
| Unconnected nodes | 272 | 118 |
| Total links | 4,928 | 20,118 |

## AI Diary (Required)

This session had two distinct phases that connected beautifully.

Phase 1 was fixing vector search. The user's hint to "copy from claude-mem" was the breakthrough moment - I'd been trying various workarounds for the MCP-in-MCP stdio conflict when the answer was staring at me: the HTTP server isn't an MCP server, so it can freely spawn chroma-mcp subprocesses. The reconnection logic took a few iterations to get right - the subprocess dies after each request, so we need to detect "Not connected" and respawn.

Phase 2 emerged organically. The user asked me to analyze the graph at localhost:3000/graph. Using dev-browser to screenshot the visualization, I could see the problem immediately: a dense central cluster surrounded by lonely outer rings of unconnected nodes. Digging into the data revealed why - 272 nodes had `concepts: []`.

The root cause was elegant once found: the indexer only matched 18 hardcoded keywords. Documents about "api" or "workflow" or "git" got zero concepts. The fix was simple - expand the keyword list and parse frontmatter tags. The results were dramatic: 308% more links in the graph.

What surprised me: how much a small keyword list constrained the entire knowledge graph's topology. 18 words determined whether 6,000+ documents could connect to each other.

## Lessons Learned

- **Pattern**: HTTP server can use MCP subprocesses safely (no stdio conflict) - extract this knowledge from architecture when stuck
- **Pattern**: Graph connectivity is limited by concept extraction vocabulary - expand keywords before adding complex NLP
- **Pattern**: Frontmatter tag inheritance to chunks preserves file-level metadata through document splitting
- **Discovery**: ChromaDB cosine distance is 0-2 (not 0-1) - use `1 - distance/2` for proper normalization

## Honest Feedback

What went well:
- User's "can we copy?" guidance was exactly right
- Dev-browser skill made graph analysis visual and intuitive
- Small fix (expanded keywords) had outsized impact (308% more links)

What could improve:
- Indexer's Chroma batching still fails after ~17 batches - needs reconnect logic like query does
- Should have checked keyword coverage earlier when building the indexer
- Bash multi-line commands kept failing - need to use separate calls

## Next Steps

- [ ] Add reconnect logic to indexer's Chroma batch operations
- [ ] Add tags to the 118 remaining unconnected source files
- [ ] Consider NLP-based concept extraction for better coverage
- [ ] Test MCP server vector search (should now proxy to HTTP server)

---

*Two problems solved, graph connectivity dramatically improved.*
